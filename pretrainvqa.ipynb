{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3081988,"sourceType":"datasetVersion","datasetId":1864860},{"sourceId":11464528,"sourceType":"datasetVersion","datasetId":7184157},{"sourceId":12623641,"sourceType":"datasetVersion","datasetId":7975977}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport random\nfrom datasets import Dataset\nimport torch\nimport torch.nn as nn\nfrom transformers import CLIPVisionModel, CLIPProcessor, AutoModelForCausalLM, AutoTokenizer, CLIPImageProcessor , GPT2LMHeadModel, GPT2Tokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom tqdm import tqdm\nimport json\n\n\ndevice = 'cuda'\n\n# Paths\ncsv_path = \"/kaggle/input/stanford-image-paragraph-captioning-dataset/stanford_df_rectified.csv\"\nimage_folder = \"/kaggle/input/stanford-image-paragraph-captioning-dataset/stanford_img/content/stanford_images\"\n\n# Load and prepare dataset\ndf = pd.read_csv(csv_path)\ndf = df.rename(columns={\"Image_name\": \"image_name\", \"Paragraph\": \"caption\"})\ndf[\"image_name\"] = df[\"image_name\"].astype(str).apply(lambda x: x if x.endswith(\".jpg\") else x + \".jpg\")\ndf[\"image_path\"] = df[\"image_name\"].apply(lambda name: os.path.join(image_folder, name))\ndf = df[df[\"caption\"].notna() & df[\"image_path\"].apply(os.path.exists)]\ndf = df.reset_index(drop=True)\n\n# Initialize processors\nclip_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\ngpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ngpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token  # Set pad token to eos token\n\nbrief_instructions = [\n    \"Describe the image concisely.\",\n    \"Provide a brief description of the given image.\",\n    \"Offer a succinct explanation of the picture presented.\",\n    \"Summarize the visual content of the image.\",\n    \"Give a short and clear explanation of the subsequent image.\",\n]\n\nclass VisionTextDataset(Dataset):\n    def __init__(self, dataframe, image_processor, tokenizer, max_length=256):\n        self.data = dataframe\n        self.image_processor = image_processor\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        image = Image.open(item[\"image_path\"]).convert(\"RGB\")\n        pixel_values = self.image_processor(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n        \n        instruction = random.choice(brief_instructions)\n        text = f\"User: {instruction}\\nAssistant: {item['caption']}\"\n        \n        tokenized = self.tokenizer(\n            text,\n            return_tensors=\"pt\",\n            max_length=self.max_length,\n            truncation=True,\n            padding=\"max_length\"\n        )\n        \n        return {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": tokenized[\"input_ids\"][0],\n            \"attention_mask\": tokenized[\"attention_mask\"][0]\n        }\n\ndef collate_fn(batch):\n    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    labels = input_ids.clone()\n    \n    return {\n        \"pixel_values\": pixel_values,\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\n# Create dataset and dataloader\ndataset = VisionTextDataset(\n    dataframe=df,\n    image_processor=clip_processor,\n    tokenizer=gpt2_tokenizer,\n    max_length=256\n)\n\npretrain_dataloader = DataLoader(\n    dataset,\n    batch_size=8,  # Increased batch size since we're using float32\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=2,\n    pin_memory=True\n)\n\nclass VisionTextModel(nn.Module):\n    def __init__(self, vision_encoder, llm, projection_in_dim, projection_out_dim, num_image_tokens=16):\n        super().__init__()\n        self.vision_encoder = vision_encoder\n        self.llm = llm\n        self.projection = nn.Sequential(\n            nn.Linear(projection_in_dim, num_image_tokens * projection_out_dim),\n            nn.LayerNorm(num_image_tokens * projection_out_dim)\n        )\n        self.num_image_tokens = num_image_tokens\n        self.set_trainable_components(vision=False, llm=False, projection=True)\n\n    def set_trainable_components(self, vision=False, llm=False, projection=True):\n        for param in self.vision_encoder.parameters():\n            param.requires_grad = vision\n        for param in self.llm.parameters():\n            param.requires_grad = llm\n        for param in self.projection.parameters():\n            param.requires_grad = projection\n        self.vision_trainable = vision\n        self.llm_trainable = llm\n\n    def forward(self, images, input_ids, attention_mask=None, labels=None):\n        # Process images\n        if self.vision_trainable:\n            vision_output = self.vision_encoder(images)\n        else:\n            with torch.no_grad():\n                vision_output = self.vision_encoder(images)\n        \n        # Extract features\n        image_features = vision_output.last_hidden_state.mean(dim=1)\n        \n        # Project features\n        projected = self.projection(image_features)\n        projected = projected.view(-1, self.num_image_tokens, self.projection[0].out_features // self.num_image_tokens)\n        projected = projected * 0.1  # Stabilize training\n\n        # Process text embeddings\n        if self.llm_trainable:\n            text_embeds = self.llm.get_input_embeddings()(input_ids)\n        else:\n            with torch.no_grad():\n                text_embeds = self.llm.get_input_embeddings()(input_ids)\n\n        # Combine embeddings\n        inputs_embeds = torch.cat([projected, text_embeds], dim=1)\n\n        # Adjust attention mask and labels\n        if attention_mask is not None:\n            image_mask = torch.ones(attention_mask.shape[0], self.num_image_tokens, device=attention_mask.device)\n            attention_mask = torch.cat([image_mask, attention_mask], dim=1)\n        \n        if labels is not None:\n            image_labels = torch.full((labels.shape[0], self.num_image_tokens), -100, device=labels.device)\n            labels = torch.cat([image_labels, labels], dim=1)\n\n        # Forward through LLM\n        outputs = self.llm(\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        return outputs\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T09:04:06.157214Z","iopub.execute_input":"2025-07-31T09:04:06.157481Z","iopub.status.idle":"2025-07-31T09:04:29.402978Z","shell.execute_reply.started":"2025-07-31T09:04:06.157460Z","shell.execute_reply":"2025-07-31T09:04:29.402317Z"}},"outputs":[{"name":"stderr","text":"2025-07-31 09:04:11.574209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753952651.596350     264 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753952651.602973     264 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load models in float32\nclip_vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\ngpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T09:05:01.978267Z","iopub.execute_input":"2025-07-31T09:05:01.978938Z","iopub.status.idle":"2025-07-31T09:05:03.364532Z","shell.execute_reply.started":"2025-07-31T09:05:01.978910Z","shell.execute_reply":"2025-07-31T09:05:03.363754Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# Initialize multimodal model\nmodel = VisionTextModel(\n    vision_encoder=clip_vision_model,\n    llm=gpt2_model,\n    projection_in_dim=768,  # CLIP hidden size\n    projection_out_dim=768,  # Match GPT-2 hidden size\n    num_image_tokens=32\n).to(device)\n\n# Set to pretrain mode\nmodel.set_trainable_components(vision=False, llm=False, projection=True)\n\n# Training configuration\ngradient_accumulation_steps = 4\nnum_epochs = 3\nlog_interval = 50\noptimizer = torch.optim.AdamW(model.projection.parameters(), lr=1e-4)\n\n# Training loop\ntotal_steps = num_epochs * len(pretrain_dataloader) // gradient_accumulation_steps\nprogress_bar = tqdm(total=total_steps, desc=\"Pretraining\")\nglobal_step = 0\nrunning_loss = 0.0\n\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    \n    for batch_idx, batch in enumerate(pretrain_dataloader):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        outputs = model(\n            images=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        loss = outputs.loss / gradient_accumulation_steps\n        loss.backward()\n        \n        global_step += 1\n        running_loss += loss.item() * gradient_accumulation_steps\n        \n        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            avg_loss = running_loss / global_step\n            progress_bar.set_postfix({\n                \"loss\": f\"{loss.item() * gradient_accumulation_steps:.4f}\",\n                \"avg_loss\": f\"{avg_loss:.4f}\"\n            })\n            progress_bar.update(1)\n        \n        if global_step % log_interval == 0:\n            tqdm.write(f\"Step {global_step}/{total_steps}: Loss={loss.item() * gradient_accumulation_steps:.4f}\")\n\nprogress_bar.close()\nprint(\"Pretraining completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T09:05:07.209511Z","iopub.execute_input":"2025-07-31T09:05:07.209821Z","iopub.status.idle":"2025-07-31T09:06:40.992820Z","shell.execute_reply.started":"2025-07-31T09:05:07.209797Z","shell.execute_reply":"2025-07-31T09:06:40.991803Z"}},"outputs":[{"name":"stderr","text":"Pretraining:   0%|          | 0/1834 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nPretraining:   1%|          | 12/1834 [00:13<30:18,  1.00it/s, loss=1.3543, avg_loss=4.5391]","output_type":"stream"},{"name":"stdout","text":"Step 50/1834: Loss=1.1988\n","output_type":"stream"},{"name":"stderr","text":"Pretraining:   1%|▏         | 25/1834 [00:25<29:57,  1.01it/s, loss=1.1464, avg_loss=2.8059]","output_type":"stream"},{"name":"stdout","text":"Step 100/1834: Loss=1.1464\n","output_type":"stream"},{"name":"stderr","text":"Pretraining:   2%|▏         | 37/1834 [00:38<29:45,  1.01it/s, loss=0.7277, avg_loss=2.2304]","output_type":"stream"},{"name":"stdout","text":"Step 150/1834: Loss=0.9491\n","output_type":"stream"},{"name":"stderr","text":"Pretraining:   3%|▎         | 50/1834 [00:50<29:32,  1.01it/s, loss=0.8466, avg_loss=1.9056]","output_type":"stream"},{"name":"stdout","text":"Step 200/1834: Loss=0.8466\n","output_type":"stream"},{"name":"stderr","text":"Pretraining:   3%|▎         | 62/1834 [01:02<29:20,  1.01it/s, loss=0.9459, avg_loss=1.7093]","output_type":"stream"},{"name":"stdout","text":"Step 250/1834: Loss=0.8466\n","output_type":"stream"},{"name":"stderr","text":"Pretraining:   4%|▍         | 75/1834 [01:15<29:07,  1.01it/s, loss=0.9481, avg_loss=1.5635]","output_type":"stream"},{"name":"stdout","text":"Step 300/1834: Loss=0.9481\n","output_type":"stream"},{"name":"stderr","text":"Pretraining:   5%|▍         | 87/1834 [01:27<28:56,  1.01it/s, loss=0.9049, avg_loss=1.4609]","output_type":"stream"},{"name":"stdout","text":"Step 350/1834: Loss=1.0372\n","output_type":"stream"},{"name":"stderr","text":"Pretraining:   5%|▌         | 92/1834 [01:32<28:50,  1.01it/s, loss=0.9192, avg_loss=1.4259]","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_264/2418239704.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"name":"stderr","text":"Pretraining:   5%|▌         | 92/1834 [01:33<29:33,  1.02s/it, loss=0.9192, avg_loss=1.4259]","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"output_dir = \"/kaggle/working/projection_pretrained_model\"\nos.makedirs(output_dir, exist_ok=True)\n\ntorch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\ngpt2_tokenizer.save_pretrained(output_dir)\nclip_processor.save_pretrained(output_dir)\n\n\nconfig = {\n    \"projection_in_dim\": 768,\n    \"projection_out_dim\": 768,\n    \"num_image_tokens\": 32\n}\n\nwith open(os.path.join(output_dir, \"model_config.json\"), \"w\") as f:\n    json.dump(config, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
